import tiktoken
import os
from LLM_utils.prompter import PromptBase
import transformers

class Calculator:
    # Pricing per 1M input tokens in USD for GPT models
    GPT_input_pricing = {
        "gpt-4o": 2.50,
        "gpt-4o-2024-11-20": 2.50,
        "gpt-4o-2024-08-06": 2.50,
        "gpt-4o-2024-05-13": 5.00,
        "gpt-4o-mini": 0.15,
        "gpt-4o-mini-2024-07-18": 0.15,
        "o1-preview": 15.00,
        "o1-preview-2024-09-12": 15.00,
        "o1-mini": 3.00,
        "o1-mini-2024-09-12": 3.00,
        "chatgpt-4o-latest": 5.00,
        "gpt-4-turbo": 10.00,
        "gpt-4-turbo-2024-04-09": 10.00,
        "gpt-4": 30.00,
        "gpt-4-32k": 60.00,
        "gpt-4-0125-preview": 10.00,
        "gpt-4-1106-preview": 10.00,
        "gpt-4-vision-preview": 10.00,
        "gpt-3.5-turbo-0125": 0.50,
        "gpt-3.5-turbo-instruct": 1.50,
        "gpt-3.5-turbo-1106": 1.00,
        "gpt-3.5-turbo-0613": 1.50,
        "gpt-3.5-turbo-16k-0613": 3.00,
        "gpt-3.5-turbo-0301": 1.50,
        "davinci-002": 2.00,
        "babbage-002": 0.40,
    }

    DeepSeek_input_pricing = {
        "deepseek-chat": 0.014,
        }
    DeepSeek_output_pricing = {
        "deepseek-chat": 0.14,
        }

    # Pricing per 1M output tokens in USD for GPT models
    GPT_output_pricing = {
        "gpt-4o": 10.00,
        "gpt-4o-2024-11-20": 10.00,
        "gpt-4o-2024-08-06": 10.00,
        "gpt-4o-2024-05-13": 15.00,
        "gpt-4o-mini": 0.60,
        "gpt-4o-mini-2024-07-18": 0.60,
        "o1-preview": 60.00,
        "o1-preview-2024-09-12": 60.00,
        "o1-mini": 12.00,
        "o1-mini-2024-09-12": 12.00,
        "chatgpt-4o-latest": 15.00,
        "gpt-4-turbo": 30.00,
        "gpt-4-turbo-2024-04-09": 30.00,
        "gpt-4": 60.00,
        "gpt-4-32k": 120.00,
        "gpt-4-0125-preview": 30.00,
        "gpt-4-1106-preview": 30.00,
        "gpt-4-vision-preview": 30.00,
        "gpt-3.5-turbo-0125": 1.50,
        "gpt-3.5-turbo-instruct": 2.00,
        "gpt-3.5-turbo-1106": 2.00,
        "gpt-3.5-turbo-0613": 2.00,
        "gpt-3.5-turbo-16k-0613": 4.00,
        "gpt-3.5-turbo-0301": 2.00,
        "davinci-002": 2.00,
        "babbage-002": 0.40,
    }

    def __init__(self, model, formatted_input_sequence=None, formatted_output_sequence=None):
        self.model = model
        self.formatted_input_sequence = formatted_input_sequence
        self.formatted_output_sequence = formatted_output_sequence
        self.input_token_length = 0
        self.output_token_length = 0

    def calculate_input_token_length_GPT(self):
        """Calculate the number of tokens used by a list of messages."""
        try:
            encoding = tiktoken.encoding_for_model(self.model)
        except KeyError:
            print(
                "Warning: model not found for tokenization. Using cl100k_base encoding for cost "
                "calculation."
            )
            encoding = tiktoken.get_encoding("cl100k_base")

        tokens_per_message = 3
        tokens_per_name = 1

        num_tokens = 0
        for message in self.formatted_input_sequence:
            num_tokens += tokens_per_message
            for key, value in message.items():
                num_tokens += len(encoding.encode(value))
                if key == "name":
                    num_tokens += tokens_per_name
        num_tokens += 3  # Every reply is primed with <|start|>assistant<|message|>
        return num_tokens

    def calculate_output_token_length_GPT(self):
        """
        Calculates the number of tokens in a given output sequence.

        Parameters:
            output_sequence (str): The text output generated by the model.
            model (str): The name of the model for tokenization. Default is "gpt-3.5-turbo".

        Returns:
            int: The number of tokens in the output sequence.
        """
        try:
            # Initialize tokenizer for the specific model
            tokenizer = tiktoken.encoding_for_model(self.model)
        except KeyError:
            print("Warning: Model not found. Using cl100k_base encoding as a fallback.")
            tokenizer = tiktoken.get_encoding("cl100k_base")

        # Tokenize the output sequence
        tokens = tokenizer.encode(self.formatted_output_sequence)

        # Return the token count
        return len(tokens)

    def calculate_cost_GPT(self):
        if self.formatted_input_sequence is not None:
            self.input_token_length = self.calculate_input_token_length_GPT()
            input_cost = self.input_token_length * self.GPT_input_pricing[self.model] / 1e6
        else:
            input_cost = 0
        if self.formatted_output_sequence is not None:
            self.output_token_length = self.calculate_output_token_length_GPT()
            output_cost = self.output_token_length * self.GPT_output_pricing[self.model] / 1e6
        else:
            output_cost = 0
        return input_cost + output_cost

    def calculate_token_length_DeepSeek(self):

        tokenizer_relative_path = "tokenizers/deepseek"

        tokenizer_absolute_path = os.path.abspath(tokenizer_relative_path)

        tokenizer = transformers.AutoTokenizer.from_pretrained(
            tokenizer_absolute_path , trust_remote_code=True
            )


        if self.formatted_input_sequence is not None:
            input_sequence=PromptBase.formatted_to_string_OpenAI(self.formatted_input_sequence)
            input_tokenized = tokenizer.encode(input_sequence)
            self.input_token_length = len(input_tokenized)
        if self.formatted_output_sequence is not None:
            output_sequence=PromptBase.formatted_to_string_OpenAI(self.formatted_output_sequence)
            output_tokenized = tokenizer.encode(output_sequence)
            self.output_token_length = len(output_tokenized)

    def calculate_cost_DeepSeek(self):
        self.calculate_token_length_DeepSeek()
        cost =self.input_token_length*self.DeepSeek_input_pricing[self.model]+self.output_token_length*self.DeepSeek_output_pricing[self.model]
        cost /= 1e6
        return cost

    def calculate_input_token_length(self , input_sequence , form="list") :
        if form == "list" :
            self.formatted_input_sequence = PromptBase.list_to_formatted_OpenAI(input_sequence)
        elif form == "formatted" :
            self.formatted_input_sequence = input_sequence
        else :
            raise ValueError("Invalid form. Use 'list' or 'formatted'.")

        if self.model in self.GPT_input_pricing :
            self.input_token_length = self.calculate_input_token_length_GPT()
        elif self.model in self.DeepSeek_input_pricing :
            self.calculate_token_length_DeepSeek()
        else :
            raise ValueError(f"Model {self.model} not supported for token length calculation.")

        return self.input_token_length

    def length_limiter(self , input_sequence , limit , truncation=True ,
                       include_truncation_warning=True) :
        self.calculate_input_token_length(input_sequence , form="list")

        if self.input_token_length > limit :
            print(f"Warning: Input sequence is longer than {limit} tokens.")

            if truncation :
                # Calculate the token length of the warning message if it will be included
                warning_message = "---Warning, this information is too long and is truncated---"
                warning_token_length = self.calculate_input_token_length([warning_message] ,
                                                                         form="list") if include_truncation_warning else 0

                # Adjust the limit to account for the warning message
                adjusted_limit = limit - warning_token_length if include_truncation_warning else limit

                # Convert the input sequence to a single string for truncation
                input_string = " ".join(input_sequence)

                # Calculate the ratio of the adjusted limit to the current token count
                ratio = adjusted_limit / self.input_token_length

                # Calculate the approximate length of the truncated string
                truncated_length = int(len(input_string) * ratio)

                # Truncate the string to the calculated length
                truncated_string = input_string[:truncated_length]

                # Split the truncated string back into a list of strings, preserving the original structure
                truncated_input_sequence = []
                current_length = 0
                for sentence in input_sequence :
                    if current_length + len(sentence) + 1 <= truncated_length :
                        truncated_input_sequence.append(sentence)
                        current_length += len(sentence) + 1  # +1 for the space
                    else :
                        # Truncate the last sentence to fit within the limit
                        remaining_length = truncated_length - current_length
                        if remaining_length > 0 :
                            truncated_sentence = sentence[:remaining_length]
                            truncated_input_sequence.append(truncated_sentence)
                        break

                # Re-calculate the token length to ensure it's within the adjusted limit
                self.calculate_input_token_length(truncated_input_sequence , form="list")

                # If the token count is still over the adjusted limit, adjust further
                while self.input_token_length > adjusted_limit :
                    ratio *= 0.99  # Reduce the ratio slightly
                    truncated_length = int(len(input_string) * ratio)
                    truncated_string = input_string[:truncated_length]

                    # Rebuild the truncated input sequence
                    truncated_input_sequence = []
                    current_length = 0
                    for sentence in input_sequence :
                        if current_length + len(sentence) + 1 <= truncated_length :
                            truncated_input_sequence.append(sentence)
                            current_length += len(sentence) + 1
                        else :
                            remaining_length = truncated_length - current_length
                            if remaining_length > 0 :
                                truncated_sentence = sentence[:remaining_length]
                                truncated_input_sequence.append(truncated_sentence)
                            break

                    # Re-calculate the token length
                    self.calculate_input_token_length(truncated_input_sequence , form="list")

                # Append the truncation warning if required
                if include_truncation_warning :
                    truncated_input_sequence.append(warning_message)

                # Re-calculate the final token length including the warning
                self.calculate_input_token_length(truncated_input_sequence , form="list")

                print(f"Warning: Input sequence is truncated to be about {limit} tokens.")
                return truncated_input_sequence
            else :
                return input_sequence
        else :
            return input_sequence



if __name__ == "__main__":
    # Define the model and input sequence
    model = "gpt-3.5-turbo-0125"
    input_sequence = [
        "This is a long input sequence that needs to be truncated.",
        "Another part of the sequence that adds to the token count.",
        "This is just some additional text to make the sequence longer.",
        "The goal is to ensure the length_limiter works as expected."
    ]
    limit = 45  # Token limit for testing

    # Create an instance of the Calculator class
    calculator = Calculator(model)

    # Test the length_limiter method with the warning message
    print("Original Input Sequence:")
    print(input_sequence)
    print("\nToken Count Before Truncation:")
    input_token_length = calculator.calculate_input_token_length(input_sequence, form="list")
    print(f"Input Token Length: {input_token_length}")

    # Apply the length_limiter with the warning message
    truncated_sequence = calculator.length_limiter(input_sequence, limit, include_truncation_warning=True)

    # Print the results
    print("\nTruncated Input Sequence with Warning:")
    print(truncated_sequence)
    print("\nToken Count After Truncation:")
    calculator.calculate_input_token_length(truncated_sequence, form="list")
    print(f"Input Token Length: {calculator.input_token_length}")